# This configuration requires Tesla V100-SXM2(32GB) x 16 GPUs It takes about 2 days.
#numofgpu: 2
kd_conf:
    decoder: transformer    #ilm decoder的结构
    encoder_output_size: 512 #原las模型的encoder的输出维度
    decoder_conf:
        attention_heads: 8
        linear_units: 2048
        num_blocks: 6
        dropout_rate: 0.1
        positional_dropout_rate: 0.1
        self_attention_dropout_rate: 0.1
        src_attention_dropout_rate: 0.1
    ilme_conf:                 #ilme 的配置
        tsf_share: true        
        ilmetype: acl
        acllayers:
            - 512
            - 512
        aclactivations:    
            - relu
            - relu
            - none
    kd_factor: 0.6   #teacherlabel是 log_target-kd_factor*log_ilm
    label_smooth: 0.0   #label_smooth for target
    kdlm_init: /home/projects/12001458/yufei/espnet/egs2/librispeech/asr1/exp/downmodel/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/valid.loss.ave_10best.pth
init_param:
    - /home/projects/12001458/yufei/espnet/egs2/librispeech/asr1/exp/downmodel/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/valid.loss.ave_10best.pth  #the pretrained LM model
    - /home/projects/12001458/yufei/espnet/egs2/librispeech/asr1/exp/downmodel/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_ilme_share/2epoch.pth  #the LAS model to load internal language model,请注意，ilme_conf 必须一致

use_amp: true
lm: transformer
lm_conf:
    pos_enc: null
    embed_unit: 128
    att_unit: 512
    head: 8
    unit: 2048
    layer: 16
    dropout_rate: 0.0

# optimization related
grad_clip: 5.0
batch_type: numel
batch_bins: 80000000
accum_grad: 25
max_epoch: 200

optim: adam
optim_conf:
   lr: 0.002
scheduler: warmuplr
scheduler_conf:
   warmup_steps: 1000

best_model_criterion:
-   - valid
    - loss
    - min
keep_nbest_models: 10  # 10 is good.
